In this assignment, I added the total time lapse to measure the total execution time for the app - 
by taking advantage of python's built-in __init__ and __exit__ functions. 
With that, we can see that with the increase in the amount of data, the total execution time increases exponetially. 
Specifically, the larger the amount of data the inporportion increase in the total lapse time. 

See below. The total time lapse is a debugging command line output.

-- regular file
DEBUG:root:Total time lapse: 0.1815643.
(import_data_customers) - 3 records processed in 0.03934320000000002 seconds.
(import_data_products) - 4 records processed in 0.033248500000000014 seconds.
(import_data_rentals) - 12 records processed in 0.03315400000000002 seconds.
(import_data) - 7 records processed in 0.1253913000000001 seconds.

-- x10 file; with 10 times the data
DEBUG:root:Total time lapse: 0.26417289999999993.
(import_data_customers) - 30 records processed in 0.0647063 seconds.
(import_data_products) - 40 records processed in 0.04341100000000009 seconds.
(import_data_rentals) - 120 records processed in 0.04940650000000002 seconds.
(import_data) - 70 records processed in 0.18547769999999997 seconds.

-- x100 file; with 100 times the data
DEBUG:root:Total time lapse: 1.3217271.
(import_data_customers) - 300 records processed in 0.05704799999999999 seconds.
(import_data_products) - 400 records processed in 0.0544458000000001 seconds.
(import_data_rentals) - 1200 records processed in 0.07723449999999998 seconds.
(import_data) - 700 records processed in 0.21178170000000007 seconds.

-- x1000 file; with 1000 times the data
DEBUG:root:Total time lapse: 57.312858500000004.
(import_data_customers) - 3000 records processed in 0.12186739999999996 seconds.
(import_data_products) - 4000 records processed in 0.12599519999999997 seconds.
(import_data_rentals) - 12000 records processed in 0.29090539999999987 seconds.
(import_data) - 7000 records processed in 0.5647076999999999 seconds.

-- x10000 file; with 10000 times the data
DEBUG:root:Total time lapse: 6865.852504099999.
(import_data_customers) - 30000 records processed in 0.6542038 seconds.
(import_data_products) - 40000 records processed in 0.7671479000000001 seconds.
(import_data_rentals) - 120000 records processed in 3.2852084999999995 seconds.
(import_data) - 70000 records processed in 4.7279395 seconds.

Then, when we dig into each function, we see that the function show_rentals is the culprit 
in slowing down the performance. We examine the function and see that the code queries the rental collection using 
the product id, then queries the customer collection to find the relationship between the two collections. With a 
small amount of data, this is not obvious. But with the 10000 times of data, uhg! That's 12000 x 3000 records to loop 
through!

In relational database, we can do a join and it would be very efficient (This method is very inefficient - a quick 
search online shows that Mongodb does not have a join). Following the thoughts above, perhaps we can improve the 
show_rentals function by using a fileter in the inner loop as a quick fix. But that's outside the scope of this 
assignment. But, at least, we identified one small section of the code to optumize.

In addition, we do see the execution time increases as the amount of data increase in the database functions. However, 
It seems Mongodb is pretty efficient in loading large amounts of data.
